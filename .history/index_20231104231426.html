<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>"Neuroformer: Multimodal, Multitask Generative Pretraining for Brain Data"</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script> -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,400;1,300&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://woanderer.github.io">Antonis Antoniades</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=6N6nROAAAAAJ&hl=en">Yiyi Yu</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=W6Bqxc0AAAAJ&hl=en">Joseph Canzano</a>,
            </span>
            <span class="author-block">
              <a href="https://slslab.org">Spencer LaVere Smith</a>,
            </span>
            <span class="author-block">
              <a href="https://sites.cs.ucsb.edu/~william/">William Wang</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of California, Santa Barbara</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <i class="fas fa-envelope"></i> Corresponding author: 
              <a href="mailto:antonis@ucsb.edu">
                antonis@ucsb.edu
              </a>
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2311.00136"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/woanderer/Neuroformer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1O6T_BH9Y2gI4eLi2FbRjTVt85kMXeZN5?usp=share_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>

</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/Screenshot 2023-10-31 at 3.26.46 PM.png" alt="Description of Image" style="width:100%;">
      <h2 class="subtitle has-text-centered">
        <strong>Neuroformer</strong> is a multimodal and multitask generative pretraining framework for brain data. It aims
        to help researchers harness the power of large-scale systems neuroscience data to build better models of the brain and beyond.
      </h2>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a method to generatively pretrain transformers on multimodal and multitask
            neuronal data, called Neuroformer. To achieve this, the Neuroformer harnesses a novel spatiotemporal tokenization scheme that models individual
            neurons as tokens. We first apply our model to synthetic 
          </p>
          <p>
            Our model can generate synthetic spiking data conditioned on varied stimuli, like video
            and reward, create useful embeddings using contrastive learning, and transfer to other downstream
            tasks like predicting behavior. 
          </p>
          We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated 
          neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, 
          including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse
          with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural 
          pretraining objective, without any explicit supervision.
        </p>
          We hope that Neuroformer can tap into the imagination of both neuroscientists and machine learning researchers,
          to push the limits of systems neuroscience data in terms of building better models of the brain and beyond!
        </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    
<!--/ Model Architecture. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/model_arch_2_2.jpg" alt="Description of Image" style="width:100%;">
      <h2 class="subtitle has-text-centered">
        <strong>Model Architecture.</strong> Neuroformer tokenizes individual neurons into spatiotemporal tokens before feeding them to an auto-regressive
        transformer. The model is trained using a generative pretraining objective that includes a causal masking loss, and a temporal token loss.
      </h2>
    </div>
    <br/>
    <br/>
  </div>

  <hr style="border-top: 1px solid #ccc;">
</section>

<!-- Attention Mirrors Hubb Connectivity -->
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Attention Mirrors Hubb Connectivity</h2>

    <!-- Interpolating. -->
    <div class="columns">
      <div class="column has-text-centered">
        <img src="./static/images/validate_02.jpg"
             class="publication-image"
             alt="Description of the image"
             style="width:90%;"/>
        <p>
          A uni-modal Neuroformer model (akin to a vanilla GPT) trained on simulated data with hub connectivity
          naturally <strong>revealed the underlying connectivity pattern</strong> within its self-attention. 
        </p>
      </div>
    </div>
  </div>
</div>

<hr style="border-top: 1px solid #ccc;">
<!--/ Paper image. -->
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Attention. -->
      <div class="column">
        <h2 class="title is-3">Visual Attention</h2>
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" controls playsinline autoplay loop muted style="width: 90%; height: auto; display: block; margin: 0 auto;">
              <source src="./static/videos/test (1)_2.mp4" type="video/mp4">
            </video>
            <br/>
            <p style="text-align: center;">
              Neuroformer's attention module can attend to each neuron individually.
              Here we show the <strong>attention map over a visual scene</strong> from the point of view of a mouse,
              conditioned on its brain.
            </p>
          </div>

        </div>
        <br/>
      </div>
    </div>

    <hr style="border-top: 1px solid #ccc;">

    <!-- Simulations of Neural Activity Conidtioned on Multimodal Input. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Simulations of Neural Activity Conidtioned on Multimodal Input</h2>
    
        <!-- Interpolating. -->
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./static/images/real_data_v3.jpg"
                  class="interpolation-image"
                  alt="Interpolate start reference image."
                  style="width:90%;"/>
            <p>
              <strong>Generative spatiotemporal pretraining</strong> enables Neuroformer to generate synthetic neural activity
              conditioned on multimodal input, such as video and reward. This activity effectively captures the variability
              of the underlying neural circuit.
            </p>
          </div>
        </div>
        <br/>
      </div>
    </div>

    <hr style="border-top: 1px solid #ccc;">

    <!-- Simulataneous Multi-task Decoding. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Simulataneous Multi-task Decoding</h2>
    
        <!-- Interpolating. -->
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./static/images/regression_2.jpg"
                  class="interpolation-image"
                  alt="Interpolate start reference image."
                  style="width:90%;"/>
            <p>
              <strong>Joint multi-task training</strong> enables a single model to simultaneously decode speed and eye position.
            </p>
            <br/>
            <img src="./static/images/regression_line.jpg"
                  class="interpolation-image"
                  alt="Interpolate start reference image."
                  style="width:90%;"/>
            <p>
              <strong>Precise Speed Decoding</strong> that outperforms conventional methods.
            </p>
          </div>
        </div>
        <br/>
      </div>
    </div>

    <hr style="border-top: 1px solid #ccc;">

    <!-- Multimodal Contrastive Learning For Learning Neural Latent Embeddings -->
    <div class="columns is-centered">
      <div class="column is-half">
        <h2 class="title is-3">Contrastive Learning For Learning Multimodal Neural Latent Embeddings</h2>
        <p>
          <strong>Contrastive learning</strong> enables Neuroformer to learn useful embeddings of neural activity.
          Here we show 3-dimensional neural latent representations over time, colored by speed.
        </p>
      </div>
      <div class="column is-half has-text-centered">
        <!-- Interpolating. -->
        <img src="./static/images/3d_speed_spherical_coords.jpg"
              class="interpolation-image"
              alt="Interpolate start reference image."
              style="width: 75%;"
              />
          </div>
        <br/>
        <br/>
      </div>

    <hr style="border-top: 1px solid #ccc;">

    <!-- Multimodal Contrastive Learning For Learning Neural Latent Embeddings -->
        <div class="columns is-centered">
          <!-- Image Column -->
          <div class="column is-half has-text-centered">
            <!-- Interpolating. -->
            <video id="config_demo" controls playsinline autoplay loop muted style="width: 90%; height: auto; display: block; margin: 0 auto;">
              <source src="./static/videos/config_neuroformer.mov" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <script>
              document.addEventListener('DOMContentLoaded', function() {
                var vid = document.getElementById('config_demo');
                vid.playbackRate = 2.75;
              });
            </script>
          </div>
          <!-- Text Column -->
          <div class="column is-half">
            <h2 class="title is-3">Easily Incorporate New Modalities</h2>
            <p>
              With Neuroformer, you can easily customize your model's input
               modalities and decoding options by simply editing the configuration <strong>file</strong>. You can specify which modalities to use, whether to decode them or use them as input, and 
               in which block to include them. For more details, visit our repository <a href="https://github.com/woanderer/Neuroformer">here</a>.
            </p>
          </div>
        <br/>
        <br/>
      </div>
      
    



    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            Other great concurrent works have also explored scaling neural data training using transformers.
            <a href="https://www.biorxiv.org/content/10.1101/2023.09.18.558113v1">Neural Data Transformer 2: Multi-context Pretraining for Neural Spiking Activity</a>,
            <a href="https://arxiv.org/abs/2310.16046">A Unified, Scalable Framework for Neural Population Decoding</a>
          </p>
          <p>
            In contrast to these works, Neuroformer:
          </p>
          <ul>
            <li>Leverages generative pretraining of neural data to learn from and predict neural responses, rather than only train to decode behavior from neural activity.</li>
            <li>Scale to complex multimodal inputs like video.</li>
            <li>Incorporate multi-modal contrastive learning to align modalities and learn useful representations.</li>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{antoniades2023neuroformer,
      title={Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data}, 
      author={Antonis Antoniades and Yiyi Yu and Joseph Canzano and William Wang and Spencer LaVere Smith},
      year={2023},
      eprint={2311.00136},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website was made using the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies paper theme</a>.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
