<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>"Neuroformer: Multimodal, Multitask Generative Pretraining for Brain Data"</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:ital,wght@0,400;1,300&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://woanderer.github.io">Antonis Antoniades</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=6N6nROAAAAAJ&hl=en">Yiyi Yu</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=W6Bqxc0AAAAJ&hl=en">Joseph Canzano</a>,
            </span>
            <span class="author-block">
              <a href="https://slslab.org">Spencer LaVere Smith</a>,
            </span>
            <span class="author-block">
              <a href="https://sites.cs.ucsb.edu/~william/">William Wang</a>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of California, Santa Barbara</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/woanderer/Neuroformer"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1O6T_BH9Y2gI4eLi2FbRjTVt85kMXeZN5?usp=share_link"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/Screenshot 2023-10-31 at 3.26.46 PM.png" alt="Description of Image" style="width:100%;">
      <h2 class="subtitle has-text-centered">
        <strong>Neuroformer</strong> is a multimodal and multitask generative pretraining framework for brain data. It aims
        to help researchers harness the power of large-scale systems neuroscience data to build better models of the brain and beyond.
      </h2>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a method to generatively pretrain transformers on multimodal and multitask
            neuronal data, called Neuroformer. To achieve this, the Neuroformer harnesses a novel spatiotemporal tokenization scheme that models individual
            neurons as tokens. We first apply our model to synthetic 
          </p>
          <p>
            Our model can generate synthetic spiking data conditioned on varied stimuli, like video
            and reward, create useful embeddings using contrastive learning, and transfer to other downstream
            tasks like predicting behavior. 
          </p>
          We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated 
          neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, 
          including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse
          with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural 
          pretraining objective, without any explicit supervision.
        </p>
          We hope that Neuroformer can tap into the imagination of both neuroscientists and machine learning researchers,
          to push the limits of systems neuroscience data in terms of building better models of the brain and beyond!
        </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    
<!--/ Model Architecture. -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/model_arch_2_2.jpg" alt="Description of Image" style="width:100%;">
      <h2 class="subtitle has-text-centered">
        <strong>Model Architecture.</strong> Neuroformer tokenizes individual neurons into spatiotemporal tokens before feeding them to an auto-regressive
        transformer. The model is trained using a generative pretraining objective that includes a causal masking loss, and a temporal token loss.
      </h2>
    </div>
  </div>
</section>

<!-- Paper video. -->
<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Attention Mirrors Hubb Connectivity</h2>

    <!-- Interpolating. -->
    <div class="columns">
      <div class="column has-text-centered">
        <img src="./static/images/validate_02.jpg"
             class="publication-image"
             alt="Description of the image"
             style="width:90%;"/>
        <p>
          A uni-modal Neuroformer model (akin to a vanilla GPT) trained on simulated data with hub connectivity
          naturally revealed the underlying connectivity revealed the underlying connectivity pattern within its self-attention. 
        </p>
      </div>
    </div>
    <br/>
  </div>
</div>
<!--/ Paper image. -->
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Attention. -->
      <div class="column">
        <h2 class="title is-3">Visual Attention</h2>
        <div class="columns is-centered">
          <div class="column content">
            <video id="matting-video" controls playsinline autoplay loop muted style="width: 75%; height: auto; display: block; margin: 0 auto;">
              <source src="./static/videos/test (1)_2.mp4" type="video/mp4">
            </video>
            <p style="text-align: center;">
              Neuroformer's attention module can attend to each neuron individually.
              Here we show the <strong>attention map over a visual scene</strong> from the point of view of a mouse,
              conditioned on its brain
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Simulations of Neural Activity Conidtioned on Multimodal Input. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Simulations of Neural Activity Conidtioned on Multimodal Input</h2>
    
        <!-- Interpolating. -->
        <div class="columns">
          <div class="column has-text-centered">
            <img src="./static/images/psth + sweeps_demo_2.jpg"
                  class="interpolation-image"
                  alt="Interpolate start reference image."/>
            <p>
              <strong>Generative spatiotemporal pretraining</strong> enables Neuroformer to generate synthetic neural activity
              conditioned on multimodal input, such as video and reward. This activity effectively captures the variability
              of the underlying neural circuit.
            </p>
          </div>
        </div>
        <br/>
      </div>
    </div>

    <!-- Animation. -->
        <!-- Simulataneous Multi-task Decoding. -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Simulataneous Multi-task Decoding</h2>
        
            <!-- Interpolating. -->
            <div class="columns">
              <div class="column has-text-centered">
                <img src="./static/images/regression_2.jpg"
                     class="interpolation-image"
                     alt="Interpolate start reference image."/>
                <p>
                  Joint multi-task training allows Neuroformer to simultaneously predict the mouse's speed and eye
                </p>
              </div>
            </div>
            <br/>
          </div>
        </div>



        <!-- Re-rendering. -->
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
